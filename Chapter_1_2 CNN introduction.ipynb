{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4349a5",
   "metadata": {},
   "source": [
    "###### **Machine Learning book on Jupyter NBs**\n",
    "\n",
    "* Giuseppe Longo - LMDS (DSF - UNINA)\n",
    "* Michele delli Veneri (DIETI - UNINA)\n",
    "\n",
    "# Chapter 1 - \n",
    "## Section 2 - Introduction to CNNs\n",
    "\n",
    "**Contents:**\n",
    "* Introduction to CNN\n",
    "* Representation and compositionality\n",
    "* Fukushima model\n",
    "* Building a simple fully connected network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c395fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: py: command not found\n",
      "/bin/bash: py: command not found\n",
      "Package                            Version            \n",
      "---------------------------------- -------------------\n",
      "alabaster                          0.7.12             \n",
      "anaconda-client                    1.7.2              \n",
      "anaconda-navigator                 1.9.12             \n",
      "anaconda-project                   0.8.3              \n",
      "applaunchservices                  0.2.1              \n",
      "appnope                            0.1.0              \n",
      "appscript                          1.0.1              \n",
      "argh                               0.26.2             \n",
      "asn1crypto                         1.3.0              \n",
      "astroid                            2.3.3              \n",
      "astropy                            4.0                \n",
      "atomicwrites                       1.3.0              \n",
      "attrs                              19.3.0             \n",
      "autopep8                           1.4.4              \n",
      "Babel                              2.8.0              \n",
      "backcall                           0.1.0              \n",
      "backports.functools-lru-cache      1.6.4              \n",
      "backports.shutil-get-terminal-size 1.0.0              \n",
      "backports.tempfile                 1.0                \n",
      "backports.weakref                  1.0.post1          \n",
      "beautifulsoup4                     4.8.2              \n",
      "bitarray                           1.2.1              \n",
      "bkcharts                           0.2                \n",
      "bleach                             3.1.0              \n",
      "bokeh                              1.4.0              \n",
      "boto                               2.49.0             \n",
      "Bottleneck                         1.3.2              \n",
      "certifi                            2019.11.28         \n",
      "cffi                               1.14.0             \n",
      "chardet                            3.0.4              \n",
      "Click                              7.0                \n",
      "cloudpickle                        1.3.0              \n",
      "clyent                             1.2.2              \n",
      "colorama                           0.4.3              \n",
      "conda                              4.10.3             \n",
      "conda-build                        3.18.11            \n",
      "conda-package-handling             1.7.3              \n",
      "conda-verify                       3.4.2              \n",
      "contextlib2                        0.6.0.post1        \n",
      "cryptography                       2.8                \n",
      "cycler                             0.10.0             \n",
      "Cython                             0.29.15            \n",
      "cytoolz                            0.10.1             \n",
      "dask                               2.11.0             \n",
      "decorator                          4.4.1              \n",
      "defusedxml                         0.6.0              \n",
      "diff-match-patch                   20181111           \n",
      "distributed                        2.11.0             \n",
      "docutils                           0.16               \n",
      "entrypoints                        0.3                \n",
      "et-xmlfile                         1.0.1              \n",
      "fastcache                          1.1.0              \n",
      "filelock                           3.0.12             \n",
      "flake8                             3.7.9              \n",
      "Flask                              1.1.1              \n",
      "fsspec                             0.6.2              \n",
      "future                             0.18.2             \n",
      "gevent                             1.4.0              \n",
      "glob2                              0.7                \n",
      "gmpy2                              2.0.8              \n",
      "graphviz                           0.18               \n",
      "greenlet                           0.4.15             \n",
      "h5py                               2.10.0             \n",
      "HeapDict                           1.0.1              \n",
      "html5lib                           1.0.1              \n",
      "hypothesis                         5.5.4              \n",
      "idna                               2.8                \n",
      "imageio                            2.6.1              \n",
      "imagesize                          1.2.0              \n",
      "importlib-metadata                 1.5.0              \n",
      "intervaltree                       3.0.2              \n",
      "ipykernel                          5.1.4              \n",
      "ipython                            7.12.0             \n",
      "ipython-genutils                   0.2.0              \n",
      "ipywidgets                         7.5.1              \n",
      "isort                              4.3.21             \n",
      "itsdangerous                       1.1.0              \n",
      "jdcal                              1.4.1              \n",
      "jedi                               0.14.1             \n",
      "Jinja2                             2.11.1             \n",
      "joblib                             0.14.1             \n",
      "json5                              0.9.1              \n",
      "jsonschema                         3.2.0              \n",
      "jupyter                            1.0.0              \n",
      "jupyter-client                     5.3.4              \n",
      "jupyter-console                    6.1.0              \n",
      "jupyter-core                       4.6.1              \n",
      "jupyterlab                         1.2.6              \n",
      "jupyterlab-server                  1.0.6              \n",
      "keyring                            21.1.0             \n",
      "kiwisolver                         1.1.0              \n",
      "lazy-object-proxy                  1.4.3              \n",
      "libarchive-c                       2.8                \n",
      "lief                               0.9.0              \n",
      "llvmlite                           0.31.0             \n",
      "locket                             0.2.0              \n",
      "lxml                               4.5.0              \n",
      "MarkupSafe                         1.1.1              \n",
      "matplotlib                         3.4.3              \n",
      "mccabe                             0.6.1              \n",
      "mistune                            0.8.4              \n",
      "mkl-fft                            1.0.15             \n",
      "mkl-random                         1.1.0              \n",
      "mkl-service                        2.3.0              \n",
      "mock                               4.0.1              \n",
      "more-itertools                     8.2.0              \n",
      "mpmath                             1.1.0              \n",
      "msgpack                            0.6.1              \n",
      "multipledispatch                   0.6.0              \n",
      "navigator-updater                  0.2.1              \n",
      "nbconvert                          5.6.1              \n",
      "nbformat                           5.0.4              \n",
      "networkx                           2.4                \n",
      "nltk                               3.4.5              \n",
      "nose                               1.3.7              \n",
      "notebook                           6.0.3              \n",
      "numba                              0.48.0             \n",
      "numexpr                            2.7.1              \n",
      "numpy                              1.18.1             \n",
      "numpydoc                           0.9.2              \n",
      "olefile                            0.46               \n",
      "opencv-contrib-python              4.5.3.56           \n",
      "openpyxl                           3.0.3              \n",
      "packaging                          20.1               \n",
      "panda                              0.3.1              \n",
      "pandas                             1.0.1              \n",
      "pandocfilters                      1.4.2              \n",
      "parso                              0.5.2              \n",
      "partd                              1.1.0              \n",
      "path                               13.1.0             \n",
      "pathlib2                           2.3.5              \n",
      "pathtools                          0.1.2              \n",
      "patsy                              0.5.1              \n",
      "pep8                               1.7.1              \n",
      "pexpect                            4.8.0              \n",
      "pickleshare                        0.7.5              \n",
      "Pillow                             7.0.0              \n",
      "pip                                20.0.2             \n",
      "pkginfo                            1.5.0.1            \n",
      "pluggy                             0.13.1             \n",
      "ply                                3.11               \n",
      "prometheus-client                  0.7.1              \n",
      "prompt-toolkit                     3.0.3              \n",
      "psutil                             5.6.7              \n",
      "ptyprocess                         0.6.0              \n",
      "py                                 1.8.1              \n",
      "pycodestyle                        2.5.0              \n",
      "pycosat                            0.6.3              \n",
      "pycparser                          2.19               \n",
      "pycrypto                           2.6.1              \n",
      "pycurl                             7.43.0.5           \n",
      "pydocstyle                         4.0.1              \n",
      "pydot                              1.4.2              \n",
      "pyflakes                           2.1.1              \n",
      "Pygments                           2.5.2              \n",
      "pylint                             2.4.4              \n",
      "pyodbc                             4.0.0-unsupported  \n",
      "pyOpenSSL                          19.1.0             \n",
      "pyparsing                          2.4.6              \n",
      "pyrsistent                         0.15.7             \n",
      "PySocks                            1.7.1              \n",
      "pytest                             5.3.5              \n",
      "pytest-arraydiff                   0.3                \n",
      "pytest-astropy                     0.8.0              \n",
      "pytest-astropy-header              0.1.2              \n",
      "pytest-doctestplus                 0.5.0              \n",
      "pytest-openfiles                   0.4.0              \n",
      "pytest-remotedata                  0.3.2              \n",
      "python-dateutil                    2.8.1              \n",
      "python-jsonrpc-server              0.3.4              \n",
      "python-language-server             0.31.7             \n",
      "pytz                               2019.3             \n",
      "PyWavelets                         1.1.1              \n",
      "PyYAML                             5.3                \n",
      "pyzmq                              18.1.1             \n",
      "QDarkStyle                         2.8                \n",
      "QtAwesome                          0.6.1              \n",
      "qtconsole                          4.6.0              \n",
      "QtPy                               1.9.0              \n",
      "requests                           2.22.0             \n",
      "rope                               0.16.0             \n",
      "Rtree                              0.9.3              \n",
      "ruamel-yaml                        0.15.87            \n",
      "scikit-image                       0.16.2             \n",
      "scikit-learn                       0.22.1             \n",
      "scipy                              1.4.1              \n",
      "seaborn                            0.10.0             \n",
      "Send2Trash                         1.5.0              \n",
      "setuptools                         46.0.0.post20200309\n",
      "simplegeneric                      0.8.1              \n",
      "singledispatch                     3.4.0.3            \n",
      "six                                1.14.0             \n",
      "snowballstemmer                    2.0.0              \n",
      "sortedcollections                  1.1.2              \n",
      "sortedcontainers                   2.1.0              \n",
      "soupsieve                          1.9.5              \n",
      "Sphinx                             2.4.0              \n",
      "sphinxcontrib-applehelp            1.0.1              \n",
      "sphinxcontrib-devhelp              1.0.1              \n",
      "sphinxcontrib-htmlhelp             1.0.2              \n",
      "sphinxcontrib-jsmath               1.0.1              \n",
      "sphinxcontrib-qthelp               1.0.2              \n",
      "sphinxcontrib-serializinghtml      1.1.3              \n",
      "sphinxcontrib-websupport           1.2.0              \n",
      "spyder                             4.0.1              \n",
      "spyder-kernels                     1.8.1              \n",
      "SQLAlchemy                         1.3.13             \n",
      "statsmodels                        0.11.0             \n",
      "sympy                              1.5.1              \n",
      "tables                             3.6.1              \n",
      "tblib                              1.6.0              \n",
      "terminado                          0.8.3              \n",
      "testpath                           0.4.4              \n",
      "toolz                              0.10.0             \n",
      "tornado                            6.0.3              \n",
      "tqdm                               4.42.1             \n",
      "traitlets                          4.3.3              \n",
      "ujson                              1.35               \n",
      "unicodecsv                         0.14.1             \n",
      "urllib3                            1.25.8             \n",
      "watchdog                           0.10.2             \n",
      "wcwidth                            0.1.8              \n",
      "webencodings                       0.5.1              \n",
      "Werkzeug                           1.0.0              \n",
      "wheel                              0.34.2             \n",
      "widgetsnbextension                 3.5.1              \n",
      "wrapt                              1.11.2             \n",
      "wurlitzer                          2.0.0              \n",
      "xlrd                               1.2.0              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XlsxWriter                         1.2.7              \r\n",
      "xlwings                            0.17.1             \r\n",
      "xlwt                               1.3.0              \r\n",
      "xmltodict                          0.12.0             \r\n",
      "yapf                               0.28.0             \r\n",
      "zict                               1.0.0              \r\n",
      "zipp                               2.2.0              \r\n"
     ]
    }
   ],
   "source": [
    "!py -m pip install pydot     \n",
    "!py -m pip install GraphViz\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f05b1",
   "metadata": {},
   "source": [
    "Regular Neural Nets don’t scale well to full images. Let us exemplicate this using a small image, let us say 32x32 pxls x 3 channels. A single fully-connected neuron in a first hidden layer of a regular Neural Network would have 32*32*3 = 3072 weights. Multiply this figure for the number of neurons in the hiddel layer (let us say 64, for instance) and you end up with 3072x64 weights as imput of the first hidden layer. An image of more respectable size (but still very small for presentday standards), e.g. 300x300x3, would lead to neurons that have 300*300*3 = 270,000 weights. Clearly, full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.\n",
    "\n",
    "The way out was in Convolutional Neuran Networks or CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0238c35",
   "metadata": {},
   "source": [
    "### 1.2.1 Introduction to CNN\n",
    "\n",
    "What is the Link between Einstein's famous sentence: *the most incomprehensible thing about the universe is that it is comprehensible*, and Deep Learning? To understand this, let us start from a quote by J. Lecun, the inventor of Convolutional Neural Networks or CNNs (also Turing Laureate in 2018). \n",
    "\n",
    "From <a href=\"https://www.researchgate.net/publication/277411157_Deep_Learning\" target=\"_blank\">Le cun et al 2015</a> (emphasys is mine):\n",
    "\n",
    "*ConvNets are designed to process data that come in the form of multiple arrays, for example a colour image composed of three 2D arrays containing pixel intensities in the three colour channels. Many data modalities are in the form of multiple arrays: 1D for signals and sequences, including language; 2D for images or audio spectrograms; and 3D for video or volumetric images. There are four key ideas behind ConvNets that take advantage of the properties of natural signals: **local connections, shared weights, pooling and the use of many layers**. The architecture of a typical ConvNet is structured as a series of stages. The first few stages are composed of two types of layers: convolutional layers and pooling layers. Units in a convolutional layer are organized in **feature maps**, within which each unit is connected to local patches in the feature maps of the previous layer through a set of weights called a filter bank. The result of this local weighted sum is then passed through a non-linearity such as a ReLU. **All units in a feature map share the same filter bank. Different feature maps in a layer use different filter banks**. The reason for this architecture is twofold. First, in array data such as images, local groups of values are often highly correlated, forming distinctive local motifs that are easily detected. Second, the local statistics of images and other signals are invariant to location. In other words, if a motif can appear in one part of the image, it could appear anywhere, hence the idea of units at different locations sharing the same weights and detecting the same pattern in different parts of the array. Mathematically, the filtering operation performed by a feature map is a discrete convolution, hence the name. \n",
    "Although the role of the convolutional layer is to detect local conjunctions of features from the previous layer, the role of the pooling layer is to merge semantically similar features into one. Because the relative positions of the features forming a motif can vary somewhat, reliably detecting the motif can be done by coarse-graining the posi- tion of each feature. A typical pooling unit computes the maximum of a local patch of units in one feature map (or in a few feature maps). Neighbouring pooling units take input from patches that are shifted by more than one row or column, thereby reducing the dimension of the representation and creating an invariance to small shifts and distortions. Two or three stages of convolution, non-linearity and pooling are stacked, followed by more convolutional and fully-connected layers. Backpropagating gradients through a ConvNet is as simple as through a regular deep network, allowing all the weights in all the filter banks to be trained. \n",
    "Deep neural networks exploit the property that many natural signals are compositional hierarchies, in which higher-level features are obtained by composing lower-level ones. In images, local combinations of edges form motifs, motifs assemble into parts, and parts form objects. Similar hierarchies exist in speech and text from sounds to phones, phonemes, syllables, words and sentences. The pooling allows representations to vary very little when elements in the previ- ous layer vary in position and appearance. \n",
    "The convolutional and pooling layers in ConvNets are directly **inspired by the classic notions of simple cells and complex cells in visual neuroscience, and the overall architecture is reminiscent of the LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral pathway**. \n",
    "When ConvNet models and monkeys are shown the same picture, the activations of high-level units in the ConvNet explains half of the variance of random sets of 160 neurons in the monkey’s inferotemporal cortex. ConvNets have their roots in the <a href=\"https://en.wikipedia.org/wiki/Neocognitron\" target=\"_blank\">neocognitron</a>, the architecture of which was somewhat similar, but did not have an end-to-end supervised-learning algorithm such as backpropagation. A primitive 1D ConvNet (also called a time-delay neural net) was used for the recognition of phonemes and simple words. \n",
    "There have been numerous applications of convolutional networks going back to the early 1990s, starting with time-delay neural networks for speech recognition and document reading. The document reading system used a ConvNet trained jointly with a probabilistic model that implemented language constraints. By the late 1990s this system was reading over 10% of all the cheques in the United States. \n",
    "A number of ConvNet-based optical character recognition and handwriting recognition systems were later deployed by Microsoft. ConvNets were also experimented with in the early 1990s for object detection in natural images, including faces and hands, and for face recognition.*\n",
    "\n",
    "This short text summarizes all main aspects of DL. In what follows we shall try to understand it but, before going into the technical details, let us focus on a particular aspect which in our opinion is particularly relevant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef12aac7",
   "metadata": {},
   "source": [
    "#### 1.2.1.1 Compositionality and representation\n",
    "\n",
    "Why does a hierarchical representation of the world work? Because the world we live in (or at least our perception of it) is compositional. This point is alluded to in previous sections. In images, for example, such hierarchical nature can be observed from the fact that local pixels assemble to form simple motifs such as oriented edges. These edges in turn are assembled to form local features such as corners, T-junctions, etc. These local features are then assembled to form motifs that are even more abstract. We can keep building on these hierarchical representation to eventually form the objects we observe in the real world.\n",
    "\n",
    "<img src=\"Immagini_ANN/Imma_121.png\" alt=\"drawing\" width=\"50%\"/>\n",
    "\n",
    "This picture is from  from [Zeiler & Fergus 2013].\n",
    "\n",
    "When describing systems with limited number of degrees of freedom this can be easily understood also as an immanent property of the world, but whether this compositionality operates at all levels of the reality must still be understood (and actually DL may help in this).However, reasoning by analogy, at the lowest level of description, we have elementary particles, which assembled to form atoms, atoms together form molecules, we continue to build on this process to form materials, parts of objects and eventually full objects in the physical world.\n",
    "The compositional nature of the world might therefore be the answer to Einstein’s rhetorical question on how humans understand the world they live in: **The most incomprehensible thing about the universe is that it is comprehensible.**\n",
    "\n",
    "*Notice that: The fact that humans understand the world thanks to this compositional nature is interpreted as a conspiracy by Yann Le Cun <a href=\"https://cfcs.pku.edu.cn/english/docs/2019-10/20191010134544591063.pdf\" target=\"_blank\"> link </a>.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ddab13",
   "metadata": {},
   "source": [
    "So, is Deep Learning  rooted in the idea that our world is comprehensible and has a compositional nature? Research conducted by Simon Thorpe may support this hypothesis. He measured the speed at which the brain reacted when some images were flashed in front of the eyes by asking users to identify these images, which they were able to do successfully. This demonstrated that it takes about 100ms for humans to detect objects. Furthermore, consider the diagram below, illustrating parts of the brain annotated with the time it takes for neurons to propagate from one area to the next:\n",
    "\n",
    "<img src=\"Immagini_ANN/Imma_131.png\" alt=\"drawing\" width=\"50%\"/>\n",
    "\n",
    "Signals pass from the retina to the LGN (helps with contrast enhancement, gate control, etc.), then to the V1 primary visual cortex, V2, V4, then to the inferotemporal cortex (PIT), which is the part of the brain where categories are defined. Observations from open-brain surgery showed that if you show a human a film, neurons in the PIT will fire only when they detect certain images -- such as Sophia Loren, Brad Pitt or a person's grandmother -- and nothing else. The neural firings are invariant to things such as position, size, illumination, your grandmother's orientation, what she's wearing, etc. Furthermore, the fast reaction times with which humans were able to categorize these items -- barely enough time for a few spikes to get through -- demonstrates that it's possible to do this without additional time spent on complex recurrent computations. Rather, this is a single feed-forward process. \n",
    "\n",
    "These insights suggest that we can develop a neural network architecture which is completely feed-forward, yet still able to solve the problem of recognition, in a way that is invariant to irrelevant transformations of the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37daa32d",
   "metadata": {},
   "source": [
    "One further insight from the human brain comes from Gallant & Van Essen, whose model of the human brain illustrates two distinct pathways:\n",
    "\n",
    "\n",
    "<img src=\"Immagini_ANN/Imma_141.png\" alt=\"drawing\" width=\"50%\"/>\n",
    "Figure depicts Gallen & Van Essen's model of dorsal & ventral pathways in the brain. The right side shows the ventral pathway, which tells you what you're looking at, while the left side shows the dorsal pathway, which identifies locations, geometry, and motion. They seem fairly separate in the human (and primate) visual cortex (with a few interactions between them of course).  \n",
    "\n",
    "<img src=\"Immagini_ANN/Imma_15.png\" alt=\"drawing\" width=\"50%\"/>\n",
    "\n",
    "Hubel & Weisel's <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/pdf/jphysiol01247-0121.pdf\" target=\"_blank\"> (Hubel & Weisel's, 1962)</a> experiments with visual stimuli in cat brains, used electrodes to measure neural firings in cat brains in response to visual stimuli. They discovered that neurons in the V1 region are only sensitive to certain areas of a visual field (called \"receptive fields\"), and detect oriented edges in that area. For example, they demonstrated that if you showed the cat a vertical bar and start rotating it, at a particular angle the neuron will fire. Similarly, as the bar moves away from that angle, the activation of the neuron diminishes. These activation-selective neurons Hubel & Weisel named \"simple cells\", for their ability to detect local features. They also discovered that if you move the bar out of the receptive field, that particular neuron doesn't fire any more, but another neuron will. There are local feature detectors corresponding to all areas of the visual field, hence the idea that the human brain processes visual information as a collection of \"convolutions\".\n",
    "Another type of neuron, which they named \"complex cells\", aggregate the output of multiple simple cells within a certain area (a great book to read about their work is: <a href=\"https://books.google.it/books?id=8YrxWojxUA4C&pg=PA106&redir_esc=y#v=onepage&q&f=false\" target=\"_blank\"> Brain and Visual Perception: history of a 25 years collaboration</a>). \n",
    "\n",
    "From a DL point of view, we can think of these narural structures as computing an aggregate of the activations using a function such as maximum, sum, sum of squares, or any other function not depending on the order. These complex cells detect edges and orientations in a region, regardless of where those stimuli lie specifically within the region. In other words, they are shift-invariant with respect to small variations in positions of the input (remind Fukushima's contributions - 1982)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb1376",
   "metadata": {},
   "source": [
    "#### 1.2.1.2 Fukushima model\n",
    "<img src=\"Immagini_ANN/Imma_16.png\" alt=\"drawing\" width=\"30%\"/>\n",
    "\n",
    "Fukushima, in a 1979 paper (in Japanese) was the first to implement the idea of multiple layers of simple cells and complex cells with computer models, using a dataset of handwritten digits. Some of these feature detectors were hand-crafted or learned, though the learning used unsupervised clustering algorithms, trained separately for each layer, as backpropagation was not yet in use. \n",
    "\n",
    "Yann LeCun came in quite a few years later (1989, 1998) and implemented the same architecture, but this time trained them in a supervised setting using backpropagation. This is widely regarded as the genesis of modern convolutional neural networks. (Note: Riesenhuber at MIT in 1999 also re-discovered this architecture, though he didn't use backpropagation.)\n",
    "Le Cun used the simple/complex cell hierarchy combined with supervised training and backpropagation to develope the first CNN at University of Toronto in ‘88-‘89 . The experiments used a small dataset of 320 ‘mouser-written’ digits. Performances of the following architectures were compared:\n",
    "\n",
    "* Single FC(fully connected) Layer\n",
    "* Two FC Layers\n",
    "* Locally Connected Layers w/o shared weights\n",
    "* Constrained network w/ shared weights and local connections\n",
    "* Constrained network w/ shared weights and local connections 2 (more feature maps)\n",
    "\n",
    "The most successful networks (constrained network with shared weights) had the strongest generalizability, and form the basis for modern CNNs. After a few years, always LeCunn implemented the first modern CNN which included a new type of Layers: *the pooling layer*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e740f52",
   "metadata": {},
   "source": [
    "## Let us build a simple neural network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8003aab3",
   "metadata": {},
   "source": [
    "We shall now build a very simple network using tensorflow and keras. First of all make sure that you are in the tensorflow environment of your Conda navigator. \n",
    "We shall use this network to process handwritten digits (as in the original work by Le Cun). The data set is kn own as the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2bb712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D,Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pydot as pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d96eff",
   "metadata": {},
   "source": [
    "##### Setting some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3dc93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS= 200\n",
    "BATCH_SIZE=128\n",
    "VERBOSE=1\n",
    "NB_CLASSES = 10 \n",
    "H_HIDDEN=128\n",
    "VALIDATION_SPLIT =0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3504c",
   "metadata": {},
   "source": [
    "* EPOCHS = maximum number of iterations\n",
    "* BATCH_SIZE = number of samples you feed into your network at a time\n",
    "* VALIDATION_SPLIT = fraction of the knowledge base which is reserved for validation\n",
    "* VERBOSE =\n",
    "* N_HIDDEN ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be123566",
   "metadata": {},
   "source": [
    "##### loading and checking the NMIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d900b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "mnist=keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape[0],'train samples')\n",
    "print(x_test.shape[0],'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05eb36",
   "metadata": {},
   "source": [
    "So, we have 60000 samples for training and 10000 samples in the test set. Let us look at a sample of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60296eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 25 mnist digits from train dataset\n",
    "indexes = np.random.randint(0, x_train.shape[0], size=25)\n",
    "images = x_train[indexes]\n",
    "labels = y_train[indexes]\n",
    "\n",
    "# plot the 25 mnist digits\n",
    "plt.figure(figsize=(5,5))\n",
    "for i in range(len(indexes)):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    image = images[i]\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "# plt.savefig(\"mnist-samples.png\")\n",
    "plt.show()\n",
    "# plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9a400",
   "metadata": {},
   "source": [
    "Let us now count how many instances of each class we have in out training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of unique train labels\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Train labels: \", dict(zip(unique, counts)))\n",
    "\n",
    "# count the number of unique test labels\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"Test labels: \", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b4b77",
   "metadata": {},
   "source": [
    "This allows us to confirm that classes are almost equally represented in both train and test (and therefore that we have a balanced classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77e728",
   "metadata": {},
   "source": [
    "##### Preparing the data for the network\n",
    "\n",
    "Images need to be prepared for the ingestion in the network.\n",
    "\n",
    "1. we have 60k images 28x28 pixels. Images need to be converted to vector of 784 (28x28=784) values. \n",
    "2. The values need to be floating point and in double precision\n",
    "3. They need to be normalized between 0 and 1\n",
    "4. Target values need to be hot encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789908c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping\n",
    "RESHAPED=784\n",
    "x_train = x_train.reshape(60000,RESHAPED)\n",
    "x_test  = x_test.reshape(10000,RESHAPED)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# normalizing\n",
    "x_train /=255\n",
    "x_test /=255\n",
    "\n",
    "# one hot encoding of target values\n",
    "y_train = tf.keras.utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, NB_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc66d1",
   "metadata": {},
   "source": [
    "##### Building and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb9a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(NB_CLASSES,input_shape=(RESHAPED,),name='dense_layer',activation='softmax'))\n",
    "model.summary()\n",
    "#plot_model(model,to_file='model_10.png')\n",
    "#plt.imshow(mpimg.imread('model_10.png'))\n",
    "#plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb6b61",
   "metadata": {},
   "source": [
    "At this point, the network is created. Before running it, however, we have to select:\n",
    "* a cost (or loss, or objective) function. Tensorflow provides a rich variety of loss functions. Here you have access to the <a href=\"https://keras.io/api/losses/\" target=\"_blank\"> complete list of options</a>.\n",
    "* optimizer: the algorithm used to update the weights at each iteration in order to find the minimum of the loss function. Tensorflow provides an extensive choice of optimizers (check here for a <a href=\"https://keras.io/api/optimizers/\" target=\"_blank\"> complete list</a>).\n",
    "* a criterion to evaluate performances (also known as metrics).\n",
    "\n",
    "In the following example we shall use the Stochastic Gradient Descent as optimizer, the categorical crossentropy as loss (cost) function, and the accuracy as metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a7ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff97cfe",
   "metadata": {},
   "source": [
    "##### training the model \n",
    "We need to define other parameters:\n",
    "* epochs: the number of times that the model is exposed to the entire training set.The optimization algorithm at the end of eache epoch corrects the weights.\n",
    "* batch_size: number of training instances observed before a weight update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eef61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train,batch_size=BATCH_SIZE,epochs=EPOCHS, verbose=VERBOSE,validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6f588c",
   "metadata": {},
   "source": [
    "##### Evaluating the results\n",
    "It is worth noticing that the accuracy (and the loss) at some poch start improving by smaller and smaller amounts. This is what we call *convergence*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3face",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_accuracy=model.evaluate(x_test,y_test)\n",
    "print('Loss: ',test_loss)\n",
    "print('Accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1602eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f66f944",
   "metadata": {},
   "source": [
    "#### A more complex architecture\n",
    "\n",
    "Let us see if adding an additional layer (and hence a larger number of neurons and weights) can improve the results.\n",
    "For convenience we repeat all steps of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7e666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D,Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pydot as pydot\n",
    "EPOCHS= 50\n",
    "BATCH_SIZE=128\n",
    "VERBOSE=1\n",
    "NB_CLASSES = 10 \n",
    "N_HIDDEN=128\n",
    "VALIDATION_SPLIT =0.2\n",
    "# load dataset\n",
    "mnist=keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape[0],'train samples')\n",
    "print(x_test.shape[0],'test samples')\n",
    "# reshaping\n",
    "RESHAPED=784\n",
    "x_train = x_train.reshape(60000,RESHAPED)\n",
    "x_test  = x_test.reshape(10000,RESHAPED)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# normalizing\n",
    "x_train /=255\n",
    "x_test /=255\n",
    "# one hot encoding of target values\n",
    "y_train = tf.keras.utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c4d274",
   "metadata": {},
   "source": [
    "##### building a more complex network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c8a0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-22 09:53:18.855132: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-22 09:53:18.855398: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN, input_shape=(RESHAPED,),name='dense_layer',activation='relu'))\n",
    "model.add(keras.layers.Dense(N_HIDDEN,name='dense_layer_2',activation='relu'))\n",
    "model.add(keras.layers.Dense(NB_CLASSES,name='dense_layer_3',activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c620d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb14d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 10s 202us/sample - loss: 1.4937 - accuracy: 0.6231 - val_loss: 0.7660 - val_accuracy: 0.8312\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 8s 164us/sample - loss: 0.6079 - accuracy: 0.8483 - val_loss: 0.4610 - val_accuracy: 0.8812\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 8s 169us/sample - loss: 0.4412 - accuracy: 0.8815 - val_loss: 0.3759 - val_accuracy: 0.8982\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 7s 155us/sample - loss: 0.3784 - accuracy: 0.8946 - val_loss: 0.3364 - val_accuracy: 0.9050\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 8s 169us/sample - loss: 0.3432 - accuracy: 0.9023 - val_loss: 0.3114 - val_accuracy: 0.9119\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 11s 227us/sample - loss: 0.3188 - accuracy: 0.9095 - val_loss: 0.2942 - val_accuracy: 0.9165\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 9s 183us/sample - loss: 0.2998 - accuracy: 0.9144 - val_loss: 0.2787 - val_accuracy: 0.9204\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 9s 181us/sample - loss: 0.2847 - accuracy: 0.9176 - val_loss: 0.2674 - val_accuracy: 0.9245\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 10s 215us/sample - loss: 0.2716 - accuracy: 0.9219 - val_loss: 0.2567 - val_accuracy: 0.9274\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 9s 181us/sample - loss: 0.2605 - accuracy: 0.9252 - val_loss: 0.2481 - val_accuracy: 0.9287\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 10s 215us/sample - loss: 0.2502 - accuracy: 0.9281 - val_loss: 0.2391 - val_accuracy: 0.9324\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 15s 313us/sample - loss: 0.2408 - accuracy: 0.9305 - val_loss: 0.2317 - val_accuracy: 0.9344\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 16s 338us/sample - loss: 0.2324 - accuracy: 0.9331 - val_loss: 0.2247 - val_accuracy: 0.9354\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 15s 304us/sample - loss: 0.2244 - accuracy: 0.9356 - val_loss: 0.2186 - val_accuracy: 0.9378\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 11s 231us/sample - loss: 0.2175 - accuracy: 0.9370 - val_loss: 0.2127 - val_accuracy: 0.9401\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 10s 198us/sample - loss: 0.2106 - accuracy: 0.9395 - val_loss: 0.2067 - val_accuracy: 0.9419\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 12s 243us/sample - loss: 0.2041 - accuracy: 0.9410 - val_loss: 0.2028 - val_accuracy: 0.9445\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 11s 222us/sample - loss: 0.1982 - accuracy: 0.9436 - val_loss: 0.1973 - val_accuracy: 0.9454\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 9s 190us/sample - loss: 0.1926 - accuracy: 0.9444 - val_loss: 0.1930 - val_accuracy: 0.9466\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 9s 194us/sample - loss: 0.1873 - accuracy: 0.9465 - val_loss: 0.1892 - val_accuracy: 0.9488\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 9s 187us/sample - loss: 0.1821 - accuracy: 0.9479 - val_loss: 0.1846 - val_accuracy: 0.9502\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 9s 185us/sample - loss: 0.1774 - accuracy: 0.9492 - val_loss: 0.1810 - val_accuracy: 0.9510\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 10s 216us/sample - loss: 0.1726 - accuracy: 0.9508 - val_loss: 0.1782 - val_accuracy: 0.9523\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 9s 188us/sample - loss: 0.1685 - accuracy: 0.9514 - val_loss: 0.1740 - val_accuracy: 0.9535\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 11s 235us/sample - loss: 0.1643 - accuracy: 0.9530 - val_loss: 0.1712 - val_accuracy: 0.9532\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 10s 199us/sample - loss: 0.1603 - accuracy: 0.9541 - val_loss: 0.1681 - val_accuracy: 0.9551\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 10s 210us/sample - loss: 0.1564 - accuracy: 0.9556 - val_loss: 0.1656 - val_accuracy: 0.9549\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 10s 215us/sample - loss: 0.1529 - accuracy: 0.9562 - val_loss: 0.1640 - val_accuracy: 0.9556\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 10s 218us/sample - loss: 0.1493 - accuracy: 0.9575 - val_loss: 0.1603 - val_accuracy: 0.9566\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 11s 220us/sample - loss: 0.1459 - accuracy: 0.9588 - val_loss: 0.1576 - val_accuracy: 0.9577\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 11s 231us/sample - loss: 0.1427 - accuracy: 0.9590 - val_loss: 0.1556 - val_accuracy: 0.9579\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 8s 171us/sample - loss: 0.1396 - accuracy: 0.9604 - val_loss: 0.1525 - val_accuracy: 0.9588\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 10s 208us/sample - loss: 0.1368 - accuracy: 0.9611 - val_loss: 0.1509 - val_accuracy: 0.9588\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 10s 217us/sample - loss: 0.1337 - accuracy: 0.9620 - val_loss: 0.1498 - val_accuracy: 0.9593\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 11s 231us/sample - loss: 0.1312 - accuracy: 0.9629 - val_loss: 0.1472 - val_accuracy: 0.9595\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 13s 262us/sample - loss: 0.1284 - accuracy: 0.9642 - val_loss: 0.1451 - val_accuracy: 0.9600\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 10s 216us/sample - loss: 0.1258 - accuracy: 0.9645 - val_loss: 0.1430 - val_accuracy: 0.9611\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 8s 168us/sample - loss: 0.1235 - accuracy: 0.9649 - val_loss: 0.1411 - val_accuracy: 0.9614\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 11s 220us/sample - loss: 0.1211 - accuracy: 0.9659 - val_loss: 0.1394 - val_accuracy: 0.9610\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 11s 229us/sample - loss: 0.1186 - accuracy: 0.9668 - val_loss: 0.1393 - val_accuracy: 0.9613\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 8s 165us/sample - loss: 0.1163 - accuracy: 0.9673 - val_loss: 0.1363 - val_accuracy: 0.9624\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 10s 209us/sample - loss: 0.1141 - accuracy: 0.9682 - val_loss: 0.1347 - val_accuracy: 0.9625\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 9s 181us/sample - loss: 0.1122 - accuracy: 0.9685 - val_loss: 0.1338 - val_accuracy: 0.9628\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 10s 212us/sample - loss: 0.1100 - accuracy: 0.9694 - val_loss: 0.1326 - val_accuracy: 0.9627\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 9s 186us/sample - loss: 0.1078 - accuracy: 0.9699 - val_loss: 0.1310 - val_accuracy: 0.9640\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 7s 153us/sample - loss: 0.1060 - accuracy: 0.9707 - val_loss: 0.1297 - val_accuracy: 0.9643\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 9s 188us/sample - loss: 0.1041 - accuracy: 0.9711 - val_loss: 0.1286 - val_accuracy: 0.9651\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 12s 244us/sample - loss: 0.1024 - accuracy: 0.9716 - val_loss: 0.1272 - val_accuracy: 0.9653\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 11s 222us/sample - loss: 0.1004 - accuracy: 0.9723 - val_loss: 0.1260 - val_accuracy: 0.9653\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 8s 158us/sample - loss: 0.0987 - accuracy: 0.9724 - val_loss: 0.1253 - val_accuracy: 0.9650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8ed8972b90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=BATCH_SIZE,epochs=EPOCHS, verbose=VERBOSE,validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d1bcb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.11789100770354272\n",
      "Accuracy: 0.9663\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy=model.evaluate(x_test,y_test,  verbose=0)\n",
    "print('Loss: ',test_loss)\n",
    "print('Accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ecc74d",
   "metadata": {},
   "source": [
    "##### Introducing Dropout"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e50ab292",
   "metadata": {},
   "source": [
    "In order to improve the results we can use a *regularization procedure* called *random dropout*, i.e. during the training we drop some weights picked up at ramdom. The general understanding is that random dropout forces the network to learn redundant patterns.\n",
    "\n",
    "As before, we shall use the same data ingestion procedure which we re-write for completeness and for the sake of clarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c9970b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D,Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "#import pydot as pydot\n",
    "#import graphviz as graphviz\n",
    "import datetime\n",
    "%load_ext tensorboard\n",
    "EPOCHS= 200\n",
    "BATCH_SIZE=128\n",
    "VERBOSE=1\n",
    "NB_CLASSES = 10 \n",
    "N_HIDDEN=128\n",
    "VALIDATION_SPLIT =0.2\n",
    "DROPOUT = 0.3\n",
    "# load dataset\n",
    "mnist=keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape[0],'train samples')\n",
    "print(x_test.shape[0],'test samples')\n",
    "# reshaping\n",
    "RESHAPED=784\n",
    "x_train = x_train.reshape(60000,RESHAPED)\n",
    "x_test  = x_test.reshape(10000,RESHAPED)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# normalizing\n",
    "x_train /=255\n",
    "x_test /=255\n",
    "# one hot encoding of target values\n",
    "y_train = tf.keras.utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b02c1",
   "metadata": {},
   "source": [
    "##### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f31228f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN,input_shape=(RESHAPED,),name='dense_layer',activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(N_HIDDEN,name='dense_layer_2',activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(NB_CLASSES,name='dense_layer_3',activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "#plot_model(model,to_file=\"model_2.png\")\n",
    "#plt.imshow(mpimg.imread('model_2.png'))\n",
    "#plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e8bd3",
   "metadata": {},
   "source": [
    "##### Compiling and running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35409a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbdffc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a19a136d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      " 1152/48000 [..............................] - ETA: 1:13 - loss: 2.3622 - accuracy: 0.1085"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-22 10:09:47.164801: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 12s 251us/sample - loss: 1.7097 - accuracy: 0.4514 - val_loss: 0.9006 - val_accuracy: 0.8115\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 10s 215us/sample - loss: 0.9190 - accuracy: 0.7181 - val_loss: 0.5363 - val_accuracy: 0.8674\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 9s 183us/sample - loss: 0.6970 - accuracy: 0.7881 - val_loss: 0.4295 - val_accuracy: 0.8852\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 9s 190us/sample - loss: 0.5937 - accuracy: 0.8205 - val_loss: 0.3734 - val_accuracy: 0.8976\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 10s 199us/sample - loss: 0.5276 - accuracy: 0.8438 - val_loss: 0.3410 - val_accuracy: 0.9032\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 10s 209us/sample - loss: 0.4867 - accuracy: 0.8540 - val_loss: 0.3161 - val_accuracy: 0.9083\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 11s 235us/sample - loss: 0.4554 - accuracy: 0.8663 - val_loss: 0.2987 - val_accuracy: 0.9120\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 10s 213us/sample - loss: 0.4282 - accuracy: 0.8747 - val_loss: 0.2842 - val_accuracy: 0.9162\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 12s 243us/sample - loss: 0.4067 - accuracy: 0.8811 - val_loss: 0.2710 - val_accuracy: 0.9203\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 10s 210us/sample - loss: 0.3898 - accuracy: 0.8855 - val_loss: 0.2593 - val_accuracy: 0.9240\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 11s 221us/sample - loss: 0.3720 - accuracy: 0.8888 - val_loss: 0.2490 - val_accuracy: 0.9268\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 10s 209us/sample - loss: 0.3631 - accuracy: 0.8922 - val_loss: 0.2407 - val_accuracy: 0.9296\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 11s 230us/sample - loss: 0.3495 - accuracy: 0.8992 - val_loss: 0.2328 - val_accuracy: 0.9297\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 11s 238us/sample - loss: 0.3380 - accuracy: 0.9006 - val_loss: 0.2248 - val_accuracy: 0.9332\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 11s 225us/sample - loss: 0.3249 - accuracy: 0.9046 - val_loss: 0.2181 - val_accuracy: 0.9358\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 13s 272us/sample - loss: 0.3138 - accuracy: 0.9088 - val_loss: 0.2120 - val_accuracy: 0.9375\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 13s 264us/sample - loss: 0.3065 - accuracy: 0.9106 - val_loss: 0.2059 - val_accuracy: 0.9386\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 13s 281us/sample - loss: 0.3006 - accuracy: 0.9103 - val_loss: 0.2008 - val_accuracy: 0.9405\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 14s 296us/sample - loss: 0.2940 - accuracy: 0.9149 - val_loss: 0.1960 - val_accuracy: 0.9422\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 13s 273us/sample - loss: 0.2869 - accuracy: 0.9162 - val_loss: 0.1922 - val_accuracy: 0.9433\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 8s 169us/sample - loss: 0.2781 - accuracy: 0.9188 - val_loss: 0.1881 - val_accuracy: 0.9443\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 9s 198us/sample - loss: 0.2733 - accuracy: 0.9202 - val_loss: 0.1845 - val_accuracy: 0.9453\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 10s 210us/sample - loss: 0.2678 - accuracy: 0.9209 - val_loss: 0.1798 - val_accuracy: 0.9471\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 8s 171us/sample - loss: 0.2631 - accuracy: 0.9226 - val_loss: 0.1768 - val_accuracy: 0.9481\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 9s 187us/sample - loss: 0.2561 - accuracy: 0.9254 - val_loss: 0.1722 - val_accuracy: 0.9497\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 11s 232us/sample - loss: 0.2512 - accuracy: 0.9262 - val_loss: 0.1696 - val_accuracy: 0.9502\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 8s 177us/sample - loss: 0.2465 - accuracy: 0.9285 - val_loss: 0.1662 - val_accuracy: 0.9515\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 9s 190us/sample - loss: 0.2423 - accuracy: 0.9279 - val_loss: 0.1636 - val_accuracy: 0.9515\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 9s 186us/sample - loss: 0.2382 - accuracy: 0.9302 - val_loss: 0.1606 - val_accuracy: 0.9528\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 10s 217us/sample - loss: 0.2350 - accuracy: 0.9313 - val_loss: 0.1584 - val_accuracy: 0.9534\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 9s 189us/sample - loss: 0.2290 - accuracy: 0.9325 - val_loss: 0.1566 - val_accuracy: 0.9542\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 10s 200us/sample - loss: 0.2259 - accuracy: 0.9333 - val_loss: 0.1537 - val_accuracy: 0.9557\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 9s 195us/sample - loss: 0.2212 - accuracy: 0.9339 - val_loss: 0.1519 - val_accuracy: 0.9557\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 9s 190us/sample - loss: 0.2173 - accuracy: 0.9366 - val_loss: 0.1497 - val_accuracy: 0.9566\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 9s 197us/sample - loss: 0.2158 - accuracy: 0.9365 - val_loss: 0.1477 - val_accuracy: 0.9564\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 11s 237us/sample - loss: 0.2146 - accuracy: 0.9372 - val_loss: 0.1454 - val_accuracy: 0.9573\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 16s 341us/sample - loss: 0.2088 - accuracy: 0.9385 - val_loss: 0.1432 - val_accuracy: 0.9586\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 17s 353us/sample - loss: 0.2069 - accuracy: 0.9385 - val_loss: 0.1411 - val_accuracy: 0.9586\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 10s 199us/sample - loss: 0.2018 - accuracy: 0.9395 - val_loss: 0.1394 - val_accuracy: 0.9591\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 10s 202us/sample - loss: 0.2021 - accuracy: 0.9406 - val_loss: 0.1377 - val_accuracy: 0.9598\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 9s 180us/sample - loss: 0.1973 - accuracy: 0.9418 - val_loss: 0.1363 - val_accuracy: 0.9599\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 8s 162us/sample - loss: 0.1942 - accuracy: 0.9432 - val_loss: 0.1352 - val_accuracy: 0.9607\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 9s 186us/sample - loss: 0.1928 - accuracy: 0.9430 - val_loss: 0.1330 - val_accuracy: 0.9607\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 9s 183us/sample - loss: 0.1899 - accuracy: 0.9440 - val_loss: 0.1322 - val_accuracy: 0.9613\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 8s 161us/sample - loss: 0.1862 - accuracy: 0.9449 - val_loss: 0.1299 - val_accuracy: 0.9622\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 8s 176us/sample - loss: 0.1857 - accuracy: 0.9446 - val_loss: 0.1287 - val_accuracy: 0.9625\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 10s 200us/sample - loss: 0.1821 - accuracy: 0.9454 - val_loss: 0.1282 - val_accuracy: 0.9623\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 11s 230us/sample - loss: 0.1803 - accuracy: 0.9477 - val_loss: 0.1267 - val_accuracy: 0.9630\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 11s 226us/sample - loss: 0.1809 - accuracy: 0.9465 - val_loss: 0.1247 - val_accuracy: 0.9635\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 8s 174us/sample - loss: 0.1762 - accuracy: 0.9479 - val_loss: 0.1233 - val_accuracy: 0.9643\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 9s 188us/sample - loss: 0.1773 - accuracy: 0.9480 - val_loss: 0.1225 - val_accuracy: 0.9644\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 8s 161us/sample - loss: 0.1738 - accuracy: 0.9491 - val_loss: 0.1217 - val_accuracy: 0.9649\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 8s 162us/sample - loss: 0.1738 - accuracy: 0.9484 - val_loss: 0.1209 - val_accuracy: 0.9646\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 9s 180us/sample - loss: 0.1706 - accuracy: 0.9497 - val_loss: 0.1196 - val_accuracy: 0.9646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 8s 169us/sample - loss: 0.1683 - accuracy: 0.9502 - val_loss: 0.1191 - val_accuracy: 0.9648\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 8s 172us/sample - loss: 0.1685 - accuracy: 0.9500 - val_loss: 0.1176 - val_accuracy: 0.9649\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 7s 156us/sample - loss: 0.1669 - accuracy: 0.9507 - val_loss: 0.1161 - val_accuracy: 0.9655\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 8s 164us/sample - loss: 0.1645 - accuracy: 0.9512 - val_loss: 0.1163 - val_accuracy: 0.9652\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 9s 178us/sample - loss: 0.1633 - accuracy: 0.9525 - val_loss: 0.1150 - val_accuracy: 0.9659\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 9s 190us/sample - loss: 0.1575 - accuracy: 0.9541 - val_loss: 0.1151 - val_accuracy: 0.9650\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 10s 218us/sample - loss: 0.1608 - accuracy: 0.9517 - val_loss: 0.1139 - val_accuracy: 0.9663\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 9s 190us/sample - loss: 0.1596 - accuracy: 0.9531 - val_loss: 0.1126 - val_accuracy: 0.9664\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 8s 173us/sample - loss: 0.1551 - accuracy: 0.9530 - val_loss: 0.1119 - val_accuracy: 0.9664\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 9s 190us/sample - loss: 0.1534 - accuracy: 0.9555 - val_loss: 0.1111 - val_accuracy: 0.9666\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 9s 196us/sample - loss: 0.1539 - accuracy: 0.9539 - val_loss: 0.1101 - val_accuracy: 0.9666\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 11s 221us/sample - loss: 0.1532 - accuracy: 0.9541 - val_loss: 0.1095 - val_accuracy: 0.9670\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 9s 179us/sample - loss: 0.1497 - accuracy: 0.9563 - val_loss: 0.1090 - val_accuracy: 0.9676\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 11s 220us/sample - loss: 0.1485 - accuracy: 0.9561 - val_loss: 0.1081 - val_accuracy: 0.9675\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 8s 168us/sample - loss: 0.1491 - accuracy: 0.9555 - val_loss: 0.1082 - val_accuracy: 0.9674\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 15s 305us/sample - loss: 0.1474 - accuracy: 0.9564 - val_loss: 0.1069 - val_accuracy: 0.9678\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 16s 343us/sample - loss: 0.1439 - accuracy: 0.9574 - val_loss: 0.1063 - val_accuracy: 0.9682\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 6s 125us/sample - loss: 0.1436 - accuracy: 0.9574 - val_loss: 0.1061 - val_accuracy: 0.9675\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 6s 132us/sample - loss: 0.1436 - accuracy: 0.9575 - val_loss: 0.1052 - val_accuracy: 0.9688\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 7s 136us/sample - loss: 0.1406 - accuracy: 0.9582 - val_loss: 0.1041 - val_accuracy: 0.9689\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 6s 128us/sample - loss: 0.1408 - accuracy: 0.9576 - val_loss: 0.1033 - val_accuracy: 0.9685\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 6s 120us/sample - loss: 0.1420 - accuracy: 0.9570 - val_loss: 0.1032 - val_accuracy: 0.9685\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 14s 298us/sample - loss: 0.1375 - accuracy: 0.9589 - val_loss: 0.1028 - val_accuracy: 0.9690\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 8s 156us/sample - loss: 0.1374 - accuracy: 0.9591 - val_loss: 0.1028 - val_accuracy: 0.9690\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 6s 119us/sample - loss: 0.1358 - accuracy: 0.9598 - val_loss: 0.1016 - val_accuracy: 0.9688\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 14s 282us/sample - loss: 0.1361 - accuracy: 0.9606 - val_loss: 0.1011 - val_accuracy: 0.9698\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 30s 633us/sample - loss: 0.1333 - accuracy: 0.9607 - val_loss: 0.1011 - val_accuracy: 0.9696\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 9s 183us/sample - loss: 0.1334 - accuracy: 0.9605 - val_loss: 0.1004 - val_accuracy: 0.9696\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 9s 181us/sample - loss: 0.1316 - accuracy: 0.9605 - val_loss: 0.1004 - val_accuracy: 0.9697\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 9s 187us/sample - loss: 0.1296 - accuracy: 0.9614 - val_loss: 0.0996 - val_accuracy: 0.9699\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 10s 205us/sample - loss: 0.1303 - accuracy: 0.9616 - val_loss: 0.0989 - val_accuracy: 0.9707\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 10s 201us/sample - loss: 0.1288 - accuracy: 0.9614 - val_loss: 0.0984 - val_accuracy: 0.9707\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 10s 203us/sample - loss: 0.1281 - accuracy: 0.9622 - val_loss: 0.0978 - val_accuracy: 0.9704\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 12s 255us/sample - loss: 0.1273 - accuracy: 0.9627 - val_loss: 0.0977 - val_accuracy: 0.9703\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 14s 291us/sample - loss: 0.1252 - accuracy: 0.9631 - val_loss: 0.0972 - val_accuracy: 0.9708\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 11s 228us/sample - loss: 0.1254 - accuracy: 0.9634 - val_loss: 0.0968 - val_accuracy: 0.9707\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 9s 197us/sample - loss: 0.1252 - accuracy: 0.9628 - val_loss: 0.0968 - val_accuracy: 0.9707\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 10s 217us/sample - loss: 0.1242 - accuracy: 0.9630 - val_loss: 0.0957 - val_accuracy: 0.9718\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 10s 202us/sample - loss: 0.1229 - accuracy: 0.9636 - val_loss: 0.0955 - val_accuracy: 0.9718\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 10s 208us/sample - loss: 0.1224 - accuracy: 0.9635 - val_loss: 0.0948 - val_accuracy: 0.9722\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 11s 222us/sample - loss: 0.1233 - accuracy: 0.9638 - val_loss: 0.0946 - val_accuracy: 0.9727\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 10s 214us/sample - loss: 0.1205 - accuracy: 0.9640 - val_loss: 0.0943 - val_accuracy: 0.9727\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 11s 231us/sample - loss: 0.1201 - accuracy: 0.9635 - val_loss: 0.0939 - val_accuracy: 0.9719\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 11s 235us/sample - loss: 0.1195 - accuracy: 0.9634 - val_loss: 0.0934 - val_accuracy: 0.9721\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 10s 213us/sample - loss: 0.1183 - accuracy: 0.9645 - val_loss: 0.0929 - val_accuracy: 0.9722\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 11s 219us/sample - loss: 0.1171 - accuracy: 0.9651 - val_loss: 0.0934 - val_accuracy: 0.9714\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 10s 207us/sample - loss: 0.1178 - accuracy: 0.9645 - val_loss: 0.0924 - val_accuracy: 0.9726\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 11s 225us/sample - loss: 0.1142 - accuracy: 0.9659 - val_loss: 0.0923 - val_accuracy: 0.9725\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 9s 194us/sample - loss: 0.1138 - accuracy: 0.9670 - val_loss: 0.0927 - val_accuracy: 0.9723\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 8s 175us/sample - loss: 0.1140 - accuracy: 0.9660 - val_loss: 0.0917 - val_accuracy: 0.9728\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 9s 180us/sample - loss: 0.1133 - accuracy: 0.9655 - val_loss: 0.0921 - val_accuracy: 0.9727\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 8s 161us/sample - loss: 0.1131 - accuracy: 0.9665 - val_loss: 0.0915 - val_accuracy: 0.9731\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 9s 178us/sample - loss: 0.1122 - accuracy: 0.9665 - val_loss: 0.0913 - val_accuracy: 0.9730\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 9s 181us/sample - loss: 0.1119 - accuracy: 0.9660 - val_loss: 0.0909 - val_accuracy: 0.9732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 9s 190us/sample - loss: 0.1108 - accuracy: 0.9671 - val_loss: 0.0913 - val_accuracy: 0.9727\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 8s 174us/sample - loss: 0.1108 - accuracy: 0.9670 - val_loss: 0.0906 - val_accuracy: 0.9726\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 12s 246us/sample - loss: 0.1097 - accuracy: 0.9667 - val_loss: 0.0897 - val_accuracy: 0.9734\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 12s 256us/sample - loss: 0.1096 - accuracy: 0.9677 - val_loss: 0.0894 - val_accuracy: 0.9737\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 8s 174us/sample - loss: 0.1083 - accuracy: 0.9670 - val_loss: 0.0897 - val_accuracy: 0.9734\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 12s 246us/sample - loss: 0.1060 - accuracy: 0.9683 - val_loss: 0.0889 - val_accuracy: 0.9737\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 12s 247us/sample - loss: 0.1089 - accuracy: 0.9684 - val_loss: 0.0888 - val_accuracy: 0.9732\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 6s 131us/sample - loss: 0.1084 - accuracy: 0.9680 - val_loss: 0.0892 - val_accuracy: 0.9733\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 6s 134us/sample - loss: 0.1063 - accuracy: 0.9683 - val_loss: 0.0883 - val_accuracy: 0.9741\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 7s 148us/sample - loss: 0.1064 - accuracy: 0.9680 - val_loss: 0.0883 - val_accuracy: 0.9737\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 6s 127us/sample - loss: 0.1050 - accuracy: 0.9674 - val_loss: 0.0882 - val_accuracy: 0.9739\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 7s 150us/sample - loss: 0.1069 - accuracy: 0.9671 - val_loss: 0.0877 - val_accuracy: 0.9743\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 7s 141us/sample - loss: 0.1051 - accuracy: 0.9688 - val_loss: 0.0880 - val_accuracy: 0.9748\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 6s 128us/sample - loss: 0.1026 - accuracy: 0.9687 - val_loss: 0.0875 - val_accuracy: 0.9745\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 7s 138us/sample - loss: 0.1036 - accuracy: 0.9683 - val_loss: 0.0877 - val_accuracy: 0.9743\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 6s 135us/sample - loss: 0.1028 - accuracy: 0.9688 - val_loss: 0.0872 - val_accuracy: 0.9748\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 7s 139us/sample - loss: 0.1005 - accuracy: 0.9702 - val_loss: 0.0870 - val_accuracy: 0.9748\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 7s 136us/sample - loss: 0.1020 - accuracy: 0.9695 - val_loss: 0.0869 - val_accuracy: 0.9737\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 6s 129us/sample - loss: 0.0991 - accuracy: 0.9694 - val_loss: 0.0867 - val_accuracy: 0.9745\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 7s 142us/sample - loss: 0.0985 - accuracy: 0.9702 - val_loss: 0.0864 - val_accuracy: 0.9739\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 8s 169us/sample - loss: 0.0984 - accuracy: 0.9705 - val_loss: 0.0865 - val_accuracy: 0.9747\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 8s 168us/sample - loss: 0.0968 - accuracy: 0.9701 - val_loss: 0.0857 - val_accuracy: 0.9746\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 9s 177us/sample - loss: 0.0987 - accuracy: 0.9698 - val_loss: 0.0861 - val_accuracy: 0.9740\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 8s 167us/sample - loss: 0.0958 - accuracy: 0.9714 - val_loss: 0.0854 - val_accuracy: 0.9745\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 7s 156us/sample - loss: 0.0984 - accuracy: 0.9707 - val_loss: 0.0851 - val_accuracy: 0.9754\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 6s 132us/sample - loss: 0.0974 - accuracy: 0.9708 - val_loss: 0.0852 - val_accuracy: 0.9750\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 7s 148us/sample - loss: 0.0968 - accuracy: 0.9709 - val_loss: 0.0855 - val_accuracy: 0.9739\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 7s 149us/sample - loss: 0.0952 - accuracy: 0.9709 - val_loss: 0.0854 - val_accuracy: 0.9750\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 7s 155us/sample - loss: 0.0970 - accuracy: 0.9705 - val_loss: 0.0848 - val_accuracy: 0.9749\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 7s 152us/sample - loss: 0.0929 - accuracy: 0.9712 - val_loss: 0.0848 - val_accuracy: 0.9753\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 7s 139us/sample - loss: 0.0960 - accuracy: 0.9709 - val_loss: 0.0845 - val_accuracy: 0.9747\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 7s 136us/sample - loss: 0.0964 - accuracy: 0.9713 - val_loss: 0.0838 - val_accuracy: 0.9753\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 7s 143us/sample - loss: 0.0954 - accuracy: 0.9705 - val_loss: 0.0841 - val_accuracy: 0.9747\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 7s 143us/sample - loss: 0.0917 - accuracy: 0.9722 - val_loss: 0.0844 - val_accuracy: 0.9749\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 7s 141us/sample - loss: 0.0927 - accuracy: 0.9726 - val_loss: 0.0840 - val_accuracy: 0.9748\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 8s 158us/sample - loss: 0.0930 - accuracy: 0.9711 - val_loss: 0.0836 - val_accuracy: 0.9753\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 6s 131us/sample - loss: 0.0934 - accuracy: 0.9718 - val_loss: 0.0844 - val_accuracy: 0.9750\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 7s 136us/sample - loss: 0.0912 - accuracy: 0.9723 - val_loss: 0.0840 - val_accuracy: 0.9748\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 7s 136us/sample - loss: 0.0912 - accuracy: 0.9717 - val_loss: 0.0834 - val_accuracy: 0.9759\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 8s 176us/sample - loss: 0.0896 - accuracy: 0.9722 - val_loss: 0.0833 - val_accuracy: 0.9756\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 7s 155us/sample - loss: 0.0895 - accuracy: 0.9723 - val_loss: 0.0832 - val_accuracy: 0.9758\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 8s 157us/sample - loss: 0.0890 - accuracy: 0.9728 - val_loss: 0.0835 - val_accuracy: 0.9759\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 6s 126us/sample - loss: 0.0904 - accuracy: 0.9726 - val_loss: 0.0827 - val_accuracy: 0.9758\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 7s 141us/sample - loss: 0.0883 - accuracy: 0.9729 - val_loss: 0.0823 - val_accuracy: 0.9757\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 7s 144us/sample - loss: 0.0883 - accuracy: 0.9726 - val_loss: 0.0822 - val_accuracy: 0.9762\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 6s 133us/sample - loss: 0.0895 - accuracy: 0.9724 - val_loss: 0.0827 - val_accuracy: 0.9756\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 7s 154us/sample - loss: 0.0859 - accuracy: 0.9739 - val_loss: 0.0825 - val_accuracy: 0.9758\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 7s 137us/sample - loss: 0.0874 - accuracy: 0.9736 - val_loss: 0.0821 - val_accuracy: 0.9762\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 7s 147us/sample - loss: 0.0874 - accuracy: 0.9737 - val_loss: 0.0820 - val_accuracy: 0.9759\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 7s 143us/sample - loss: 0.0862 - accuracy: 0.9737 - val_loss: 0.0817 - val_accuracy: 0.9765\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 7s 156us/sample - loss: 0.0855 - accuracy: 0.9740 - val_loss: 0.0818 - val_accuracy: 0.9763\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 7s 147us/sample - loss: 0.0839 - accuracy: 0.9737 - val_loss: 0.0813 - val_accuracy: 0.9765\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 8s 162us/sample - loss: 0.0847 - accuracy: 0.9752 - val_loss: 0.0820 - val_accuracy: 0.9767\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 7s 154us/sample - loss: 0.0858 - accuracy: 0.9741 - val_loss: 0.0815 - val_accuracy: 0.9767\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 7s 153us/sample - loss: 0.0852 - accuracy: 0.9737 - val_loss: 0.0812 - val_accuracy: 0.9765\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 7s 146us/sample - loss: 0.0845 - accuracy: 0.9738 - val_loss: 0.0815 - val_accuracy: 0.9768\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 8s 169us/sample - loss: 0.0831 - accuracy: 0.9744 - val_loss: 0.0812 - val_accuracy: 0.9763\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 7s 138us/sample - loss: 0.0841 - accuracy: 0.9738 - val_loss: 0.0809 - val_accuracy: 0.9762\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 7s 153us/sample - loss: 0.0819 - accuracy: 0.9754 - val_loss: 0.0815 - val_accuracy: 0.9762\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 6s 135us/sample - loss: 0.0829 - accuracy: 0.9743 - val_loss: 0.0812 - val_accuracy: 0.9765\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 6s 134us/sample - loss: 0.0821 - accuracy: 0.9751 - val_loss: 0.0814 - val_accuracy: 0.9764\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 6s 125us/sample - loss: 0.0854 - accuracy: 0.9741 - val_loss: 0.0811 - val_accuracy: 0.9767\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 6s 133us/sample - loss: 0.0796 - accuracy: 0.9759 - val_loss: 0.0807 - val_accuracy: 0.9765\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 6s 128us/sample - loss: 0.0839 - accuracy: 0.9734 - val_loss: 0.0803 - val_accuracy: 0.9762\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 6s 123us/sample - loss: 0.0837 - accuracy: 0.9741 - val_loss: 0.0807 - val_accuracy: 0.9764\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 7s 137us/sample - loss: 0.0823 - accuracy: 0.9749 - val_loss: 0.0800 - val_accuracy: 0.9768\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 6s 124us/sample - loss: 0.0815 - accuracy: 0.9747 - val_loss: 0.0801 - val_accuracy: 0.9769\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 6s 128us/sample - loss: 0.0808 - accuracy: 0.9756 - val_loss: 0.0795 - val_accuracy: 0.9767\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 8s 168us/sample - loss: 0.0805 - accuracy: 0.9757 - val_loss: 0.0805 - val_accuracy: 0.9762\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 7s 152us/sample - loss: 0.0790 - accuracy: 0.9757 - val_loss: 0.0799 - val_accuracy: 0.9767\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 6s 129us/sample - loss: 0.0783 - accuracy: 0.9756 - val_loss: 0.0801 - val_accuracy: 0.9768\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 6s 123us/sample - loss: 0.0791 - accuracy: 0.9754 - val_loss: 0.0801 - val_accuracy: 0.9769\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0766 - accuracy: 0.9764 - val_loss: 0.0799 - val_accuracy: 0.9765\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 7s 146us/sample - loss: 0.0782 - accuracy: 0.9762 - val_loss: 0.0800 - val_accuracy: 0.9768\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 7s 139us/sample - loss: 0.0765 - accuracy: 0.9766 - val_loss: 0.0805 - val_accuracy: 0.9768\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 7s 138us/sample - loss: 0.0787 - accuracy: 0.9755 - val_loss: 0.0796 - val_accuracy: 0.9768\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.0772 - accuracy: 0.9764 - val_loss: 0.0803 - val_accuracy: 0.9765\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 6s 133us/sample - loss: 0.0814 - accuracy: 0.9744 - val_loss: 0.0792 - val_accuracy: 0.9767\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 6s 132us/sample - loss: 0.0766 - accuracy: 0.9759 - val_loss: 0.0793 - val_accuracy: 0.9770\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 7s 147us/sample - loss: 0.0772 - accuracy: 0.9766 - val_loss: 0.0792 - val_accuracy: 0.9774\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 7s 145us/sample - loss: 0.0772 - accuracy: 0.9761 - val_loss: 0.0795 - val_accuracy: 0.9776\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 6s 124us/sample - loss: 0.0758 - accuracy: 0.9767 - val_loss: 0.0788 - val_accuracy: 0.9778\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 6s 134us/sample - loss: 0.0752 - accuracy: 0.9766 - val_loss: 0.0795 - val_accuracy: 0.9767\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 6s 128us/sample - loss: 0.0729 - accuracy: 0.9773 - val_loss: 0.0789 - val_accuracy: 0.9774\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 7s 151us/sample - loss: 0.0769 - accuracy: 0.9766 - val_loss: 0.0794 - val_accuracy: 0.9772\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 7s 139us/sample - loss: 0.0756 - accuracy: 0.9769 - val_loss: 0.0789 - val_accuracy: 0.9777\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 6s 129us/sample - loss: 0.0736 - accuracy: 0.9764 - val_loss: 0.0786 - val_accuracy: 0.9773\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 6s 124us/sample - loss: 0.0755 - accuracy: 0.9770 - val_loss: 0.0787 - val_accuracy: 0.9772\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 6s 125us/sample - loss: 0.0725 - accuracy: 0.9780 - val_loss: 0.0786 - val_accuracy: 0.9777\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 6s 126us/sample - loss: 0.0742 - accuracy: 0.9770 - val_loss: 0.0781 - val_accuracy: 0.9776\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 6s 131us/sample - loss: 0.0739 - accuracy: 0.9765 - val_loss: 0.0785 - val_accuracy: 0.9769\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 6s 125us/sample - loss: 0.0725 - accuracy: 0.9777 - val_loss: 0.0790 - val_accuracy: 0.9775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8eb8459b10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=BATCH_SIZE,epochs=EPOCHS, verbose=VERBOSE,validation_split=VALIDATION_SPLIT,callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7eb7cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_15484/2721476051.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy=model.evaluate(x_test,y_test, verbose=0)\n",
    "print('Loss: ',test_loss)\n",
    "print('Accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa39d36",
   "metadata": {},
   "source": [
    "#### what happens:\n",
    "<img src=\"Immagini_ANN/Imma_21.gif\" alt=\"drawing\" width=\"20%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e5b9d4",
   "metadata": {},
   "source": [
    "##### Saving a model\n",
    "\n",
    "Once you have trained a model you want to save the weight configuration so that you can apply to data not belonging to the training set for which you want to predict the target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c9309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bf8bb88",
   "metadata": {},
   "source": [
    "##### Saving the weights of a model\n",
    "\n",
    "Once you have trained a model you want to save the weight configuration so that you can apply to data not belonging to the training set for which you want to predict the target value.You can save them either in the internal tensorflow or in the internal keras format. The second one is recommendable since it can be exported also in other frameworks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef9f446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights to a tensorflow internal format\n",
    "model.save_weights('./weights/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae04e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights to keras internal format\n",
    "model.save_weights('my_model.h5',save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50848b36",
   "metadata": {},
   "source": [
    "##### Saving the model & the weights\n",
    "\n",
    "If you wish to save the model (architecture + parameters) and the weights you must use the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cc71834",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d4110d",
   "metadata": {},
   "source": [
    "##### To load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be2766eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48cc85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
